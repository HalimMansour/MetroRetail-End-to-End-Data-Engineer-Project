# MetroRetail Data Pipeline - Project Overview

A comprehensive data pipeline for retail analytics using Apache Airflow, dbt, and SQL Server.

## ğŸ—ï¸ Architecture

```
Raw Layer (CSV) â†’ Staging Layer (dbt) â†’ Silver Layer (dbt) â†’ Gold Layer (dbt) â†’ Analytics
```

### Key Components

- **Airflow**: Orchestration & scheduling
- **dbt**: SQL transformation framework
- **SQL Server**: Data warehouse
- **Python**: Data ingestion & utilities

---

## ğŸ“ Project Structure

```
MetroRetail/
â”œâ”€â”€ dags/                          # Airflow DAGs
â”‚   â”œâ”€â”€ metro_retail_pipeline.py   # Main pipeline DAG
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ dag_config.py          # DAG configuration
â”‚
â”œâ”€â”€ pipelines/                     # Python ingestion scripts
â”‚   â”œâ”€â”€ pull_weather_data.py       # Fetch weather from API
â”‚   â”œâ”€â”€ ingest_csv.py              # Load CSV to Raw layer
â”‚   â”œâ”€â”€ config.py                  # Database config
â”‚   â”œâ”€â”€ db_utils.py                # Database utilities
â”‚   â””â”€â”€ schema.py                  # Data models
â”‚
â”œâ”€â”€ dbt/metro_dbt/                 # dbt project
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/               # Staging models (cleanse)
â”‚   â”‚   â”œâ”€â”€ silver/                # Silver models (aggregate)
â”‚   â”‚   â””â”€â”€ gold/                  # Gold models (analytics-ready)
â”‚   â”œâ”€â”€ macros/                    # dbt macros
â”‚   â”œâ”€â”€ profiles.yml               # dbt configuration
â”‚   â””â”€â”€ dbt_project.yml            # dbt project config
â”‚
â”œâ”€â”€ sqlserver/                     # SQL Server DDL
â”‚   â”œâ”€â”€ 01_create_schemas.sql      # Create Raw, Staging, Silver, Gold schemas
â”‚   â”œâ”€â”€ 02_create_raw_tables_ddl.sql
â”‚   â”œâ”€â”€ 03_test_load.sql
â”‚   â”œâ”€â”€ 04_create_staging_tables_ddl.sql
â”‚   â”œâ”€â”€ 05_stging_layer_template.sql
â”‚   â”œâ”€â”€ 06_create_silver_tables_ddl.sql
â”‚   â”œâ”€â”€ 07_create_gold_tables_ddl.sql
â”‚   â””â”€â”€ validation_checklist/
â”‚       â””â”€â”€ master_staging_validation.sql
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample/                    # Sample CSV files
â”‚       â”œâ”€â”€ erp_products.csv
â”‚       â”œâ”€â”€ erp_stores.csv
â”‚       â”œâ”€â”€ erp_inventory.csv
â”‚       â”œâ”€â”€ crm_customers.csv
â”‚       â”œâ”€â”€ mkt_promotions.csv
â”‚       â”œâ”€â”€ pos_transactions_header.csv
â”‚       â”œâ”€â”€ pos_transactions_lines.csv
â”‚       â””â”€â”€ api_weather.csv
â”‚
â”œâ”€â”€ airflow_home/                  # Airflow configuration
â”‚   â”œâ”€â”€ airflow.cfg                # Airflow settings
â”‚   â””â”€â”€ dags/                      # Symbolic link to dags/
â”‚
â”œâ”€â”€ logs/                          # Pipeline logs
â”‚
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ .env.sample                    # Environment variables template
â”œâ”€â”€ README.md                      # Quick start guide
â”œâ”€â”€ WSL2_SETUP.md                  # WSL2 setup instructions
â”œâ”€â”€ start_airflow_wsl2.ps1         # Start Airflow in WSL2
â”œâ”€â”€ stop_airflow_wsl2.ps1          # Stop Airflow in WSL2
â””â”€â”€ init_airflow.sh                # Initialize Airflow environment
```

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10+
- SQL Server Express or Standard
- WSL2 (recommended for Airflow on Windows)
- Windows ODBC Driver 17 for SQL Server

### 1. Setup Environment

```powershell
# Clone/download the project
cd C:\Work\Projects\MetroRetail

# Create virtual environment
python -m venv .venv

# Activate virtual environment
.\.venv\Scripts\Activate.ps1

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure Database

Update `.env` with your SQL Server details:
```
SQL_SERVER=10.255.255.254\SQLEXPRESS
SQL_DATABASE=MetroRetailDB
SQL_USERNAME=halim
SQL_PASSWORD=Halim@1999!
```

Run SQL Server setup scripts:
```powershell
# Execute scripts in order (in SQL Server Management Studio)
sqlserver/01_create_schemas.sql
sqlserver/02_create_raw_tables_ddl.sql
sqlserver/04_create_staging_tables_ddl.sql
sqlserver/06_create_silver_tables_ddl.sql
sqlserver/07_create_gold_tables_ddl.sql
```

### 3. Start Airflow (WSL2)

```powershell
# Start Airflow in WSL2 Ubuntu
.\start_airflow_wsl2.ps1

# Access Airflow UI at: http://172.29.83.242:8080
# Username: admin
# Password: admin123
```

### 4. Run the Pipeline

- Go to Airflow UI
- Find `metro_retail_pipeline` DAG
- Click "Trigger DAG" or manually trigger tasks

---

## ğŸ“Š Pipeline Stages

### Stage 1: Pull Weather Data
- Fetches weather from Open-Meteo API
- Saves to `data/sample/api_weather.csv`
- ~28,000 rows of historical weather

### Stage 2: Ingest CSV Files
- Loads all CSV files to SQL Server Raw schema
- Files processed:
  - `erp_products.csv` â†’ Raw.erp_products
  - `erp_stores.csv` â†’ Raw.erp_stores
  - `erp_inventory.csv` â†’ Raw.erp_inventory
  - `crm_customers.csv` â†’ Raw.crm_customers
  - `mkt_promotions.csv` â†’ Raw.mkt_promotions
  - `pos_transactions_header.csv` â†’ Raw.pos_transactions_header
  - `pos_transactions_lines.csv` â†’ Raw.pos_transactions_lines
  - `api_weather.csv` â†’ Raw.api_weather

### Stage 3-5: dbt Transformations
- **Staging**: Data cleansing, standardization, type conversion
- **Silver**: Business logic, aggregations, joins
- **Gold**: Star schema for analytics (facts & dimensions)

### Stage 6: Data Quality Checks
- Validates row counts
- Confirms all layers processed successfully

---

## ğŸ“š Key Files

### DAG Configuration
**[dags/config/dag_config.py](dags/config/dag_config.py)**
- DAG schedule interval
- Default arguments
- Paths to dbt, pipelines, data

### Database Utilities
**[pipelines/db_utils.py](pipelines/db_utils.py)**
- Connection management
- Bulk insert operations
- Manifest tracking

### dbt Models
- **Staging**: Clean raw data, fix data types
- **Silver**: Aggregate, join related tables
- **Gold**: Create fact/dimension tables for BI tools

### SQL Server
- Raw schema: Load staging area
- Staging schema: Cleansed data
- Silver schema: Business-ready aggregations
- Gold schema: Star schema analytics layer

---

## ğŸ”§ Configuration

### Environment Variables (.env)
```
SQL_SERVER=10.255.255.254\SQLEXPRESS
SQL_DATABASE=MetroRetailDB
SQL_USERNAME=halim
SQL_PASSWORD=your_password
```

### Airflow Settings (airflow_home/airflow.cfg)
```
executor = SequentialExecutor
dags_folder = /mnt/c/Work/Projects/MetroRetail/dags
database = sqlite
```

### dbt Profiles (dbt/metro_dbt/profiles.yml)
```yaml
metro_dbt:
  target: dev
  outputs:
    dev:
      type: sqlserver
      server: 10.255.255.254
      database: MetroRetailDB
      schema: Staging
```

---

## ğŸ“Š Data Model

### Staging Layer (SQL Server)
- Clean, standardized versions of raw data
- Data type conversions
- Null handling
- Deduplication

### Silver Layer
- Business logic applied
- Aggregations calculated
- Related data joined
- Performance optimized

### Gold Layer (Star Schema)
**Fact Tables:**
- `fact_sales`: Transaction details with amounts, quantities
- `fact_inventory_snapshot`: Inventory levels over time

**Dimension Tables:**
- `dim_customer`: Customer attributes
- `dim_product`: Product catalog
- `dim_store`: Store locations
- `dim_date`: Date dimension (time intelligence)
- `dim_promotion`: Promotion details
- `dim_weather`: Weather conditions by date/location
- `bridge_promotion_product`: Many-to-many relationship

---

## ğŸ› ï¸ Troubleshooting

### Issue: DAG not appearing in Airflow
**Solution:** Check dags_folder path in airflow.cfg is correct for WSL2

### Issue: Database connection timeout
**Solution:** WSL2 can't reach Windows SQL Server directly. Use Windows host IP (10.255.255.254) or enable TCP/IP in SQL Server

### Issue: dbt models failing
**Solution:** Verify dbt/metro_dbt/profiles.yml has correct server, database, and credentials

### Issue: CSV ingestion fails
**Solution:** Ensure ODBC Driver 17 is installed in WSL2:
```bash
sudo ACCEPT_EULA=Y apt-get install -y msodbcsql17
```

---

## ğŸ“ˆ Monitoring

### Airflow UI
- Check DAG run history: http://172.29.83.242:8080/dags/metro_retail_pipeline
- View task logs: Click task â†’ Logs tab
- Check task status: Green = Success, Red = Failed

### Logs
```
logs/scheduler.log        # Scheduler logs
logs/webserver.log        # Webserver logs
airflow_home/logs/        # Task logs
```

---

## ğŸ¯ Best Practices

1. **Always use WSL2** for Airflow on Windows
2. **Run dbt commands** from dbt/metro_dbt/ directory
3. **Test CSV files** before large ingestions
4. **Monitor dbt model lineage** in dbt/metro_dbt/target/graph.gpickle
5. **Validate data** after each stage completes
6. **Keep .env secure** - don't commit to Git

---

## ğŸ“ Next Steps

1. âœ… Verify all tasks turn green in Airflow
2. âœ… Query Gold schema tables from SQL Server
3. âœ… Connect BI tool (Power BI, Tableau) to Gold schema
4. âœ… Add custom dbt models for your analytics needs
5. âœ… Schedule pipeline with cron or Airflow scheduler

---

## ğŸ“ Support

For issues or questions:
1. Check logs in `logs/` directory
2. Review dbt documentation: https://docs.getdbt.com
3. Check Airflow documentation: https://airflow.apache.org
4. Verify SQL Server connectivity with SSMS

---

**Last Updated:** January 7, 2026
